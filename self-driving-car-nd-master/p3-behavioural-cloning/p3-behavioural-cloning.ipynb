{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, SpatialDropout2D, ELU\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Cropping2D\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples:  9383\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "reduced = False\n",
    "\n",
    "if reduced == True:\n",
    "    csv_filepath = 'data-udacity/driving_log_reduced.csv'\n",
    "else:\n",
    "    csv_filepath = 'data-udacity/driving_log.csv'\n",
    "samples = []\n",
    "with open(csv_filepath) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "        \n",
    "def add_to_samples(csv_filepath, samples):\n",
    "    with open(csv_filepath) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for line in reader:\n",
    "            samples.append(line)\n",
    "    return samples\n",
    "\n",
    "samples = add_to_samples('data-recovery-annie/driving_log.csv', samples)\n",
    "\n",
    "samples = samples[1:]\n",
    "print(\"Samples: \", len(samples))    \n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                name = './data-udacity/'+batch_sample[0]\n",
    "                # name = './data-udacity/IMG/'+batch_sample[0].split('/')[-1]\n",
    "                center_image = mpimg.imread(name)\n",
    "                center_angle = float(batch_sample[3])\n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            \n",
    "            # print(\"X_train: \", X_train)\n",
    "            # print(\"y_train: \", y_train)\n",
    "            yield shuffle(X_train, y_train)\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=32)\n",
    "validation_generator = generator(validation_samples, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_comma(image):\n",
    "    import tensorflow as tf  # This import is required here otherwise the model cannot be loaded in drive.py\n",
    "    return tf.image.resize_images(image, 40, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "cropping2d_4 (Cropping2D)        (None, 65, 320, 3)    0           cropping2d_input_4[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)                (None, 40, 160, 3)    0           cropping2d_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)                (None, 40, 160, 3)    0           lambda_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 10, 40, 16)    3088        lambda_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "elu_14 (ELU)                     (None, 10, 40, 16)    0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 5, 20, 32)     12832       elu_14[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_15 (ELU)                     (None, 5, 20, 32)     0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 3, 10, 64)     51264       elu_15[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 1920)          0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "elu_16 (ELU)                     (None, 1920)          0           flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 512)           983552      elu_16[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_17 (ELU)                     (None, 512)           0           dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 1)             513         elu_17[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 1051249\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Comma.ai model\n",
    "# https://github.com/commaai/research/blob/master/train_steering_model.py\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Crop 70 pixels from the top of the image and 25 from the bottom\n",
    "model.add(Cropping2D(cropping=((70, 25), (0, 0)),\n",
    "                     dim_ordering='tf', # default\n",
    "                     input_shape=(160, 320, 3)))\n",
    "\n",
    "# Resize the data\n",
    "model.add(Lambda(resize_comma))\n",
    "\n",
    "model.add(Lambda(lambda x: (x/255.0) - 0.5))\n",
    "\n",
    "model.add(Convolution2D(16, 8, 8, subsample=(4, 4), border_mode=\"same\"))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Convolution2D(64, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(Dropout(.2))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Dense(512))\n",
    "# model.add(Dropout(.5))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "\n",
    "model.compile(optimizer=adam, loss=\"mse\", metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing reference: [Geoff Breemer](https://carnd-forums.udacity.com/questions/36045049/answers/36047341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.5383"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessica/anaconda/lib/python3.5/site-packages/keras/engine/training.py:1470: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: saving model to ./tmp/comma-4c.00-0.03.hdf5\n",
      "8924/8913 [==============================] - 46s - loss: 0.0322 - acc: 0.5388 - val_loss: 0.0304 - val_acc: 0.5132\n",
      "Epoch 2/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.5394Epoch 00001: saving model to ./tmp/comma-4c.01-0.04.hdf5\n",
      "8924/8913 [==============================] - 43s - loss: 0.0275 - acc: 0.5393 - val_loss: 0.0353 - val_acc: 0.4729\n",
      "Epoch 3/20\n",
      "8888/8913 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.5412Epoch 00002: saving model to ./tmp/comma-4c.02-0.02.hdf5\n",
      "8920/8913 [==============================] - 42s - loss: 0.0245 - acc: 0.5410 - val_loss: 0.0220 - val_acc: 0.5316\n",
      "Epoch 4/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.5416Epoch 00003: saving model to ./tmp/comma-4c.03-0.04.hdf5\n",
      "8924/8913 [==============================] - 44s - loss: 0.0230 - acc: 0.5415 - val_loss: 0.0376 - val_acc: 0.4750\n",
      "Epoch 5/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.5449Epoch 00004: saving model to ./tmp/comma-4c.04-0.02.hdf5\n",
      "8924/8913 [==============================] - 43s - loss: 0.0218 - acc: 0.5449 - val_loss: 0.0220 - val_acc: 0.5336\n",
      "Epoch 6/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.5466Epoch 00005: saving model to ./tmp/comma-4c.05-0.03.hdf5\n",
      "8924/8913 [==============================] - 66s - loss: 0.0203 - acc: 0.5470 - val_loss: 0.0332 - val_acc: 0.4833\n",
      "Epoch 7/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.5473Epoch 00006: saving model to ./tmp/comma-4c.06-0.03.hdf5\n",
      "8924/8913 [==============================] - 68s - loss: 0.0195 - acc: 0.5471 - val_loss: 0.0268 - val_acc: 0.5153\n",
      "Epoch 8/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.5500Epoch 00007: saving model to ./tmp/comma-4c.07-0.03.hdf5\n",
      "8924/8913 [==============================] - 68s - loss: 0.0185 - acc: 0.5494 - val_loss: 0.0263 - val_acc: 0.4896\n",
      "Epoch 9/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.5488Epoch 00008: saving model to ./tmp/comma-4c.08-0.03.hdf5\n",
      "8924/8913 [==============================] - 66s - loss: 0.0180 - acc: 0.5485 - val_loss: 0.0309 - val_acc: 0.5031\n",
      "Epoch 10/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.5479Epoch 00009: saving model to ./tmp/comma-4c.09-0.02.hdf5\n",
      "8924/8913 [==============================] - 70s - loss: 0.0174 - acc: 0.5483 - val_loss: 0.0234 - val_acc: 0.5214\n",
      "Epoch 11/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.5505Epoch 00010: saving model to ./tmp/comma-4c.10-0.03.hdf5\n",
      "8924/8913 [==============================] - 85s - loss: 0.0170 - acc: 0.5499 - val_loss: 0.0330 - val_acc: 0.4875\n",
      "Epoch 12/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.5515Epoch 00011: saving model to ./tmp/comma-4c.11-0.02.hdf5\n",
      "8924/8913 [==============================] - 88s - loss: 0.0162 - acc: 0.5512 - val_loss: 0.0241 - val_acc: 0.5356\n",
      "Epoch 13/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.5524Epoch 00012: saving model to ./tmp/comma-4c.12-0.04.hdf5\n",
      "8924/8913 [==============================] - 135s - loss: 0.0160 - acc: 0.5522 - val_loss: 0.0379 - val_acc: 0.4833\n",
      "Epoch 14/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.5505Epoch 00013: saving model to ./tmp/comma-4c.13-0.02.hdf5\n",
      "8924/8913 [==============================] - 156s - loss: 0.0156 - acc: 0.5503 - val_loss: 0.0242 - val_acc: 0.5295\n",
      "Epoch 15/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.5526Epoch 00014: saving model to ./tmp/comma-4c.14-0.04.hdf5\n",
      "8924/8913 [==============================] - 58s - loss: 0.0152 - acc: 0.5529 - val_loss: 0.0356 - val_acc: 0.4833\n",
      "Epoch 16/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.5518Epoch 00015: saving model to ./tmp/comma-4c.15-0.03.hdf5\n",
      "8924/8913 [==============================] - 53s - loss: 0.0152 - acc: 0.5520 - val_loss: 0.0292 - val_acc: 0.5193\n",
      "Epoch 17/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.5523Epoch 00016: saving model to ./tmp/comma-4c.16-0.03.hdf5\n",
      "8924/8913 [==============================] - 54s - loss: 0.0147 - acc: 0.5531 - val_loss: 0.0320 - val_acc: 0.4938\n",
      "Epoch 18/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.5529Epoch 00017: saving model to ./tmp/comma-4c.17-0.03.hdf5\n",
      "8924/8913 [==============================] - 53s - loss: 0.0144 - acc: 0.5530 - val_loss: 0.0320 - val_acc: 0.5132\n",
      "Epoch 19/20\n",
      "8892/8913 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.5535Epoch 00018: saving model to ./tmp/comma-4c.18-0.03.hdf5\n",
      "8924/8913 [==============================] - 51s - loss: 0.0140 - acc: 0.5543 - val_loss: 0.0281 - val_acc: 0.5083\n",
      "Epoch 20/20\n",
      "8888/8913 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.5524Epoch 00019: saving model to ./tmp/comma-4c.19-0.04.hdf5\n",
      "8920/8913 [==============================] - 52s - loss: 0.0139 - acc: 0.5525 - val_loss: 0.0360 - val_acc: 0.5042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x126a07d30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "batch_size = 32\n",
    "nb_epoch = 20\n",
    " \n",
    "checkpointer = ModelCheckpoint(filepath=\"./tmp/comma-4c.{epoch:02d}-{val_loss:.2f}.hdf5\", verbose=1, save_best_only=False)\n",
    "    \n",
    "model.fit_generator(train_generator, \n",
    "                    samples_per_epoch=len(train_samples), \n",
    "                    validation_data=validation_generator,\n",
    "                    nb_val_samples=len(validation_samples), nb_epoch=nb_epoch,\n",
    "                    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model-4b.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
